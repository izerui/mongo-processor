#!/usr/bin/env python3
import os
import platform
import shutil
import sys
import time
import concurrent.futures
import subprocess
from pymongo import MongoClient
from configparser import ConfigParser
from pathlib import Path

from typing import List, Tuple
from dump import MyDump, Mongo
from restore import MyRestore


def cleanup_dump_folder(dump_folder: Path) -> None:
    """æ¸…ç†å†å²å¯¼å‡ºç›®å½•"""
    if dump_folder.exists():
        # åªåˆ é™¤ç›®å½•å†…å®¹ï¼Œä¸åˆ é™¤ç›®å½•æœ¬èº«ï¼ˆäº‘ç›˜æŒ‚è½½è·¯å¾„ï¼‰
        for item in dump_folder.iterdir():
            if item.is_file():
                item.unlink()
            elif item.is_dir():
                shutil.rmtree(item)


def process_single_database(db_name: str, source: Mongo, target: Mongo,
                            numParallelCollections: int, numInsertionWorkersPerCollection: int, dump_folder: Path,
                            large_collection_threshold: int = 1000000) -> Tuple[str, bool, float]:
    """
    å¤„ç†å•ä¸ªæ•°æ®åº“çš„å¯¼å‡ºã€å¯¼å…¥å’Œæ¸…ç†
    :param db_name: æ•°æ®åº“åç§°
    :param source: æºMongoDBè¿æ¥ä¿¡æ¯
    :param target: ç›®æ ‡MongoDBè¿æ¥ä¿¡æ¯
    :param numParallelCollections: å¹¶å‘æ•°
    :param numInsertionWorkersPerCollection: æ¯ä¸ªé›†åˆçš„æ’å…¥å·¥ä½œçº¿ç¨‹æ•°
    :param dump_folder: å¯¼å‡ºç›®å½•
    :param large_collection_threshold: å¤§é›†åˆé˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼ä½¿ç”¨åˆ†åŒºå¯¼å‡º
    :return: (æ•°æ®åº“å, æ˜¯å¦æˆåŠŸ, æ€»è€—æ—¶)
    """
    start_time = time.time()
    try:
        # å¯¼å‡º
        export_start_time = time.time()
        mydump = MyDump(source, numParallelCollections)
        dump_dirs = mydump.export_db(database=db_name, dump_root_path=str(dump_folder), threshold_docs=large_collection_threshold)
        export_time = time.time() - export_start_time
        print(f' âœ… æˆåŠŸä»{source.host}å¯¼å‡º: {db_name} (è€—æ—¶: {export_time:.2f}ç§’)')

        # å¯¼å…¥
        import_start_time = time.time()
        myrestore = MyRestore(target, numParallelCollections, numInsertionWorkersPerCollection)
        myrestore.restore_db(database=db_name, dump_dirs=dump_dirs)
        import_time = time.time() - import_start_time
        print(f' âœ… æˆåŠŸå¯¼å…¥{target.host}: {db_name} (è€—æ—¶: {import_time:.2f}ç§’)')

        # åˆ é™¤å½“å‰æ•°æ®åº“çš„å¯¼å‡ºç›®å½•
        db_export_dir = os.path.join(str(dump_folder), db_name)
        print(f' âœ… åˆ é™¤ä¸´æ—¶æ–‡ä»¶ç¼“å­˜: {db_export_dir}')
        if os.path.exists(db_export_dir):
            shutil.rmtree(db_export_dir)

        total_time = time.time() - start_time
        return db_name, True, total_time

    except Exception as e:
        total_time = time.time() - start_time
        print(f' âŒ å¤„ç†æ•°æ®åº“ {db_name} å¤±è´¥: {str(e)}')
        return db_name, False, total_time


def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†"""
    config = ConfigParser()
    config_path = Path(__file__).parent.parent / 'config.ini'
    config.read(config_path)

    source = Mongo(
        config.get('source', 'host'),
        config.get('source', 'port'),
        config.get('source', 'username'),
        config.get('source', 'password')
    )

    target = Mongo(
        config.get('target', 'host'),
        config.get('target', 'port'),
        config.get('target', 'username'),
        config.get('target', 'password')
    )

    databases = config.get('global', 'databases').split(',')
    maxThreads = config.getint('global', 'maxThreads', fallback=4)  # æ–°å¢é…ç½®é¡¹
    numParallelCollections = config.getint('global', 'numParallelCollections')
    numInsertionWorkersPerCollection = config.getint('global', 'numInsertionWorkersPerCollection')
    large_collection_threshold = config.getint('global', 'largeCollectionThreshold', fallback=1000000)  # å¤§é›†åˆé˜ˆå€¼

    dump_folder = Path(__file__).parent.parent / 'dumps'

    # æ¸…ç†å†å²å¯¼å‡ºç›®å½•
    cleanup_dump_folder(dump_folder)
    dump_folder.mkdir(exist_ok=True)

    print(f"âš™ï¸ å¯¼å‡ºé…ç½®: å•åº“å¹¶å‘æ•°={numParallelCollections}, çº¿ç¨‹æ± å¹¶å‘æ•°={maxThreads}, å¤§é›†åˆé˜ˆå€¼={large_collection_threshold:,}")
    print(f"ğŸ“Š å¾…å¤„ç†æ•°æ®åº“: {len(databases)}ä¸ª")

    total_start_time = time.time()

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†æ¯ä¸ªæ•°æ®åº“
    successful_dbs = []
    failed_dbs = []

    with concurrent.futures.ThreadPoolExecutor(max_workers=maxThreads) as pool:
        # æäº¤æ‰€æœ‰æ•°æ®åº“å¤„ç†ä»»åŠ¡
        future_to_db = {
            pool.submit(process_single_database, db.strip(), source, target,
                        numParallelCollections, numInsertionWorkersPerCollection, dump_folder, large_collection_threshold): db.strip()
            for db in databases
        }

        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        try:
            for future in concurrent.futures.as_completed(future_to_db):
                db_name = future_to_db[future]
                try:
                    db_name, success, duration = future.result()
                    if success:
                        successful_dbs.append((db_name, duration))
                        print(f' ğŸ‰ æ•°æ®åº“ {db_name} å¤„ç†å®Œæˆ (è€—æ—¶: {duration:.2f}ç§’)')
                    else:
                        failed_dbs.append((db_name, duration))
                        print(f' ğŸ’¥ æ•°æ®åº“ {db_name} å¤„ç†å¤±è´¥ (è€—æ—¶: {duration:.2f}ç§’)')

                except KeyboardInterrupt:
                    print(f" âš ï¸  ç”¨æˆ·ä¸­æ–­å¤„ç†æ•°æ®åº“: {db_name}")
                    failed_dbs.append((db_name, 0))
                    break
                except TimeoutError as e:
                    print(f" â° æ•°æ®åº“ {db_name} å¤„ç†è¶…æ—¶: {str(e)}")
                    failed_dbs.append((db_name, 0))
                except Exception as e:
                    failed_dbs.append((db_name, 0))
                    print(f' ğŸ’¥ æ•°æ®åº“ {db_name} å¤„ç†æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}')
        except KeyboardInterrupt:
            print("\n âš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")
            # å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡
            for future in future_to_db:
                future.cancel()
            print(" âœ… å·²å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡")

    total_time = time.time() - total_start_time

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“ˆ å¤„ç†å®Œæˆç»Ÿè®¡:")
    print(f"   âœ… æˆåŠŸ: {len(successful_dbs)}ä¸ªæ•°æ®åº“")
    for db_name, duration in successful_dbs:
        print(f"      - {db_name}: {duration:.2f}ç§’")

    if failed_dbs:
        print(f"   âŒ å¤±è´¥: {len(failed_dbs)}ä¸ªæ•°æ®åº“")
        for db_name, duration in failed_dbs:
            print(f"      - {db_name}: {duration:.2f}ç§’")

    print(f' ğŸ¯ æ‰€æœ‰æ•°æ®åº“æ“ä½œå®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’')

    # å¦‚æœæœ‰å¤±è´¥çš„æ•°æ®åº“ï¼Œé€€å‡ºç ä¸ºé0
    if failed_dbs:
        print("âš ï¸  éƒ¨åˆ†æ•°æ®åº“å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
        # ä¸å¼ºåˆ¶é€€å‡ºï¼Œè®©ç”¨æˆ·é€‰æ‹©æ˜¯å¦é‡è¯•
        retry = input("æ˜¯å¦é‡è¯•å¤±è´¥çš„æ•°æ®åº“ï¼Ÿ(y/n): ").lower().strip()
        if retry == 'y':
            # é‡æ–°å¤„ç†å¤±è´¥çš„æ•°æ®åº“
            failed_db_names = [db[0] for db in failed_dbs]
            print(f"é‡æ–°å¤„ç†å¤±è´¥çš„æ•°æ®åº“: {failed_db_names}")
            # è¿™é‡Œå¯ä»¥æ·»åŠ é‡è¯•é€»è¾‘

    # ç¨‹åºç»“æŸ
    print("ğŸ’¤ ç¨‹åºæ‰§è¡Œå®Œæˆï¼Œè¿›å…¥ä¼‘çœ çŠ¶æ€...")

    try:
        while True:
            time.sleep(3600)  # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
    except KeyboardInterrupt:
        print("æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œç¨‹åºç»“æŸ")
        sys.exit(0)


if __name__ == "__main__":
    main()
