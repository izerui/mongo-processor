#!/usr/bin/env python3
import argparse
import concurrent.futures
import os
import shutil
import sys
import time
from configparser import ConfigParser
from pathlib import Path
from typing import Tuple, Optional

from dump import MyDump, Mongo
from restore import MyRestore
from base import ShardConfig
from index import IndexManager, create_mongo_client


def cleanup_dump_folder(dump_folder: Path) -> None:
    """æ¸…ç†å†å²å¯¼å‡ºç›®å½•"""
    if dump_folder.exists():
        # åªåˆ é™¤ç›®å½•å†…å®¹ï¼Œä¸åˆ é™¤ç›®å½•æœ¬èº«ï¼ˆäº‘ç›˜æŒ‚è½½è·¯å¾„ï¼‰
        for item in dump_folder.iterdir():
            if item.is_file():
                item.unlink()
            elif item.is_dir():
                shutil.rmtree(item)


def process_single_database(db_name: str, source: Mongo, target: Mongo,
                            numParallelCollections: int, numInsertionWorkersPerCollection: int, dump_folder: Path,
                            enable_sharding: bool = True, shard_config: Optional[ShardConfig] = None,
                            skip_export: bool = False) -> \
Tuple[str, bool, float, float, float]:
    """
    å¤„ç†å•ä¸ªæ•°æ®åº“çš„å¯¼å‡ºã€å¯¼å…¥å’Œæ¸…ç†
    :param db_name: æ•°æ®åº“åç§°
    :param source: æºMongoDBè¿æ¥ä¿¡æ¯
    :param target: ç›®æ ‡MongoDBè¿æ¥ä¿¡æ¯
    :param numParallelCollections: å¹¶å‘æ•°
    :param numInsertionWorkersPerCollection: æ¯ä¸ªé›†åˆçš„æ’å…¥å·¥ä½œçº¿ç¨‹æ•°
    :param dump_folder: å¯¼å‡ºç›®å½•
    :param enable_sharding: æ˜¯å¦å¯ç”¨åˆ†ç‰‡
    :param shard_config: åˆ†ç‰‡é…ç½®
    :param skip_export: æ˜¯å¦è·³è¿‡å¯¼å‡ºæ­¥éª¤
    :return: (æ•°æ®åº“å, æ˜¯å¦æˆåŠŸ, æ€»è€—æ—¶, å¯¼å‡ºæ—¶é—´, å¯¼å…¥æ—¶é—´)
    """
    start_time = time.time()
    export_time = 0.0
    import_time = 0.0

    try:
        # å¯¼å‡º
        export_start_time = time.time()
        if not skip_export:
            mydump = MyDump(source, numParallelCollections, enable_sharding, shard_config)
            mydump.export_db(database=db_name, dump_root_path=str(dump_folder))
            export_time = time.time() - export_start_time
            print(f' âœ… æˆåŠŸä»{source.host}å¯¼å‡º: {db_name} (è€—æ—¶: {export_time:.2f}ç§’)')
        else:
            export_time = 0.0
            print(f' â­ï¸  è·³è¿‡å¯¼å‡º: {db_name} (ä½¿ç”¨å·²æœ‰å¯¼å‡ºæ•°æ®)')

        # å¯¼å…¥
        import_start_time = time.time()
        myrestore = MyRestore(target, numParallelCollections, numInsertionWorkersPerCollection)
        myrestore.restore_db(database=db_name, dump_root_path=str(dump_folder))
        import_time = time.time() - import_start_time
        print(f' âœ… æˆåŠŸå¯¼å…¥{target.host}: {db_name} (è€—æ—¶: {import_time:.2f}ç§’)')

        total_time = time.time() - start_time
        return db_name, True, total_time, export_time, import_time

    except Exception as e:
        total_time = time.time() - start_time
        print(f' âŒ å¤„ç†æ•°æ®åº“ {db_name} å¤±è´¥: {str(e)}')
        return db_name, False, total_time, export_time, import_time


def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='MongoDBæ•°æ®è¿ç§»å·¥å…·')
    parser.add_argument('--skip-export', action='store_true',
                       help='è·³è¿‡å¯¼å‡ºæ­¥éª¤ï¼Œç›´æ¥ä½¿ç”¨å·²æœ‰å¯¼å‡ºæ•°æ®è¿›è¡Œå¯¼å…¥')
    parser.add_argument('--config', type=str, default='config.ini',
                       help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config.ini)')

    args = parser.parse_args()

    config = ConfigParser()
    config_path = Path(__file__).parent.parent / args.config
    config.read(config_path)

    source = Mongo(
        config.get('source', 'host'),
        config.get('source', 'port'),
        config.get('source', 'username'),
        config.get('source', 'password')
    )

    target = Mongo(
        config.get('target', 'host'),
        config.get('target', 'port'),
        config.get('target', 'username'),
        config.get('target', 'password')
    )

    databases = config.get('global', 'databases').split(',')
    maxThreads = config.getint('global', 'maxThreads', fallback=4)  # æ–°å¢é…ç½®é¡¹
    numParallelCollections = config.getint('global', 'numParallelCollections')
    numInsertionWorkersPerCollection = config.getint('global', 'numInsertionWorkersPerCollection')
    # å‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆçº§é«˜äºé…ç½®æ–‡ä»¶
    skip_export = args.skip_export or config.getboolean('global', 'skipExport', fallback=False)

    # åˆ†ç‰‡ç›¸å…³é…ç½®
    enable_sharding = config.getboolean('global', 'enableSharding', fallback=True)
    min_documents_for_shard = config.getint('global', 'minDocumentsForShard', fallback=1000000)
    default_shard_count = config.getint('global', 'defaultShardCount', fallback=4)
    max_shard_count = config.getint('global', 'maxShardCount', fallback=16)

    # åˆ›å»ºåˆ†ç‰‡é…ç½®
    shard_config = ShardConfig()
    shard_config.min_documents_for_shard = min_documents_for_shard
    shard_config.default_shard_count = default_shard_count
    shard_config.max_shard_count = max_shard_count

    dump_folder = Path(__file__).parent.parent / 'dumps'

    # æ¸…ç†å†å²å¯¼å‡ºç›®å½•ï¼ˆä»…åœ¨éœ€è¦å¯¼å‡ºæ—¶ï¼‰
    if not skip_export:
        cleanup_dump_folder(dump_folder)
        dump_folder.mkdir(exist_ok=True)
    else:
        print("âš ï¸  è·³è¿‡å¯¼å‡ºæ¨¡å¼ï¼Œä¿ç•™ç°æœ‰å¯¼å‡ºæ•°æ®")

    print(f"âš™ï¸ å¯¼å‡ºé…ç½®: å•åº“å¹¶å‘æ•°={numParallelCollections}, çº¿ç¨‹æ± å¹¶å‘æ•°={maxThreads}, è·³è¿‡å¯¼å‡º={skip_export}")
    print(f"ğŸ”„ åˆ†ç‰‡é…ç½®: å¯ç”¨åˆ†ç‰‡={enable_sharding}, åˆ†ç‰‡é˜ˆå€¼={min_documents_for_shard:,}æ¡, æœ€å¤§åˆ†ç‰‡æ•°={max_shard_count}")
    print(f"ğŸ“Š å¾…å¤„ç†æ•°æ®åº“: {len(databases)}ä¸ª")

    total_start_time = time.time()

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†æ¯ä¸ªæ•°æ®åº“
    successful_dbs = []
    failed_dbs = []

    with concurrent.futures.ThreadPoolExecutor(max_workers=maxThreads) as pool:
        # æäº¤æ‰€æœ‰æ•°æ®åº“å¤„ç†ä»»åŠ¡
        future_to_db = {
            pool.submit(process_single_database, db.strip(), source, target,
                        numParallelCollections, numInsertionWorkersPerCollection, dump_folder,
                        enable_sharding, shard_config, skip_export): db.strip()
            for db in databases
        }

        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        try:
            for future in concurrent.futures.as_completed(future_to_db):
                db_name = future_to_db[future]
                try:
                    db_name, success, duration, export_time, import_time = future.result()
                    if success:
                        successful_dbs.append((db_name, duration, export_time, import_time))
                        print(f' ğŸ‰ æ•°æ®åº“ {db_name} å¤„ç†å®Œæˆ (è€—æ—¶: {duration:.2f}ç§’)')
                    else:
                        failed_dbs.append((db_name, duration, export_time, import_time))
                        print(f' ğŸ’¥ æ•°æ®åº“ {db_name} å¤„ç†å¤±è´¥ (è€—æ—¶: {duration:.2f}ç§’)')

                except KeyboardInterrupt:
                    print(f" âš ï¸  ç”¨æˆ·ä¸­æ–­å¤„ç†æ•°æ®åº“: {db_name}")
                    failed_dbs.append((db_name, 0, 0, 0))
                    break
                except TimeoutError as e:
                    print(f" â° æ•°æ®åº“ {db_name} å¤„ç†è¶…æ—¶: {str(e)}")
                    failed_dbs.append((db_name, 0, 0, 0))
                except Exception as e:
                    failed_dbs.append((db_name, 0, 0, 0))
                    print(f' ğŸ’¥ æ•°æ®åº“ {db_name} å¤„ç†æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}')
        except KeyboardInterrupt:
            print("\n âš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")
            # å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡
            for future in future_to_db:
                future.cancel()
            print(" âœ… å·²å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡")

    total_time = time.time() - total_start_time

    # è®¡ç®—æ€»è®¡æ—¶é—´
    total_export_time = sum(db[2] for db in successful_dbs) + sum(db[2] for db in failed_dbs)
    total_import_time = sum(db[3] for db in successful_dbs) + sum(db[3] for db in failed_dbs)

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“ˆ å¤„ç†å®Œæˆç»Ÿè®¡:")
    print(f"   âœ… æˆåŠŸ: {len(successful_dbs)}ä¸ªæ•°æ®åº“")
    for db_name, duration, export_time, import_time in successful_dbs:
        print(f"      - {db_name}: {duration:.2f}ç§’ (å¯¼å‡º: {export_time:.2f}ç§’, å¯¼å…¥: {import_time:.2f}ç§’)")

    if failed_dbs:
        print(f"   âŒ å¤±è´¥: {len(failed_dbs)}ä¸ªæ•°æ®åº“")
        for db_name, duration, export_time, import_time in failed_dbs:
            print(f"      - {db_name}: {duration:.2f}ç§’ (å¯¼å‡º: {export_time:.2f}ç§’, å¯¼å…¥: {import_time:.2f}ç§’)")

    print(f"\nğŸ“Š æ€»è®¡ç»Ÿè®¡:")
    print(f"   æ€»å¯¼å‡ºæ—¶é—´: {total_export_time:.2f}ç§’")
    print(f"   æ€»å¯¼å…¥æ—¶é—´: {total_import_time:.2f}ç§’")
    print(f"   æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")
    if total_export_time + total_import_time > 0:
        print(f"   å¹¶è¡Œæ•ˆç‡: {total_time / (total_export_time + total_import_time):.2f}x")
    else:
        print(f"   å¹¶è¡Œæ•ˆç‡: N/A")

    print(f' ğŸ¯ æ‰€æœ‰æ•°æ®åº“æ“ä½œå®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’')

    # å¦‚æœæœ‰å¤±è´¥çš„æ•°æ®åº“ï¼Œæ‰“å°é”™è¯¯ä¿¡æ¯
    if failed_dbs:
        print("âš ï¸  éƒ¨åˆ†æ•°æ®åº“å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
        print("   å¤±è´¥çš„æ•°æ®åº“:")
        for db_name, duration, export_time, import_time in failed_dbs:
            print(f"      - {db_name}")

    # ç¨‹åºç»“æŸ
    # ä¸ºæˆåŠŸçš„æ•°æ®åº“åˆ›å»ºç´¢å¼•
    if successful_dbs:
        print("\nğŸ” å¼€å§‹åˆ›å»ºç´¢å¼•...")
        print("=" * 50)

        try:
            # åˆ›å»ºMongoDBå®¢æˆ·ç«¯
            source_client = create_mongo_client(
                source.host,
                int(source.port),
                source.username or "",
                source.password or ""
            )

            target_client = create_mongo_client(
                target.host,
                int(target.port),
                target.username or "",
                target.password or ""
            )

            # åˆ›å»ºç´¢å¼•ç®¡ç†å™¨
            index_manager = IndexManager(source_client, target_client)

            # è·å–æˆåŠŸçš„æ•°æ®åº“ååˆ—è¡¨
            successful_db_names = [db[0] for db in successful_dbs]

            # å¹¶å‘åˆ›å»ºç´¢å¼•
            index_results = index_manager.recreate_indexes_for_databases(
                successful_db_names,
                max_workers=maxThreads
            )

            # æ‰“å°ç´¢å¼•åˆ›å»ºç»Ÿè®¡
            print("\n" + "=" * 50)
            print(f"ğŸ“Š ç´¢å¼•åˆ›å»ºæ±‡æ€»ç»Ÿè®¡:")
            print(f"   ğŸ“‚ å¤„ç†æ•°æ®åº“æ•°: {index_results['total_databases']}")
            print(f"   âœ… æˆåŠŸæ•°æ®åº“æ•°: {index_results['successful_databases']}")
            print(f"   ğŸ“ˆ æ€»ç´¢å¼•æ•°: {index_results['total_indexes']}")
            print(f"   ğŸ¯ æˆåŠŸåˆ›å»ºç´¢å¼•æ•°: {index_results['created_indexes']}")
            print(f"   â±ï¸  æ€»è€—æ—¶: {index_results['duration']:.2f}ç§’")

            if index_results['failed_databases']:
                print(f"\n   âŒ å¤±è´¥æ•°æ®åº“:")
                for failed_db in index_results['failed_databases']:
                    print(f"      ğŸ“› {failed_db['database']}: {failed_db.get('error', 'æœªçŸ¥é”™è¯¯')}")
            else:
                print(f"\n   ğŸ‰ æ‰€æœ‰æ•°æ®åº“ç´¢å¼•åˆ›å»ºæˆåŠŸ!")

            # å…³é—­å®¢æˆ·ç«¯è¿æ¥
            source_client.close()
            target_client.close()

        except Exception as e:
            print(f"âš ï¸  ç´¢å¼•åˆ›å»ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

    print("ğŸ’¤ ç¨‹åºæ‰§è¡Œå®Œæˆï¼Œè¿›å…¥ä¼‘çœ çŠ¶æ€...")

    try:
        while True:
            time.sleep(3600)  # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
    except KeyboardInterrupt:
        print("æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œç¨‹åºç»“æŸ")
        sys.exit(0)


if __name__ == "__main__":
    main()
