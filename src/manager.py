#!/usr/bin/env python3
"""
ç´¢å¼•å¤„ç†æ¨¡å—
ç”¨äºä»source MongoDBè¯»å–ç´¢å¼•ä¿¡æ¯ï¼Œå¹¶åœ¨target MongoDBåå°åˆ›å»ºç´¢å¼•
"""
import concurrent.futures
import logging
import shutil
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Any, Tuple

from src.base import GlobalConfig, MongoConfig
from src.dump import MyDump
from src.restore import MyRestore

logger = logging.getLogger(__name__)


class Manager:
    """ç´¢å¼•ç®¡ç†å™¨ï¼Œè´Ÿè´£è¯»å–å’Œåˆ›å»ºç´¢å¼•"""

    def __init__(self, source_config: MongoConfig, target_config: MongoConfig, global_config: GlobalConfig):
        """
        åˆå§‹åŒ–ç´¢å¼•ç®¡ç†å™¨

        Args:
            source_config: æºMongoDBå®¢æˆ·ç«¯
            target_config: ç›®æ ‡MongoDBå®¢æˆ·ç«¯
        """
        self.source_config = source_config
        self.target_config = target_config
        self.global_config = global_config
        self.source_mongo = MyDump(source_config, global_config)
        self.target_mongo = MyRestore(target_config, global_config)

    def process_single_database(self, db_name: str) -> Tuple[str, bool, float, float, float]:
        """
        å¤„ç†å•ä¸ªæ•°æ®åº“çš„å¯¼å‡ºã€å¯¼å…¥å’Œæ¸…ç†
        :param db_name: æ•°æ®åº“åç§°
        :return: (æ•°æ®åº“å, æ˜¯å¦æˆåŠŸ, æ€»è€—æ—¶, å¯¼å‡ºæ—¶é—´, å¯¼å…¥æ—¶é—´)
        """
        start_time = time.time()
        export_time = 0.0
        import_time = 0.0

        try:
            # å¯¼å‡º
            export_start_time = time.time()
            if not self.global_config.skip_export:
                self.source_mongo.export_db(database=db_name)
                export_time = time.time() - export_start_time
                print(f' âœ… æˆåŠŸä»{self.source_config.host}å¯¼å‡º: {db_name} (è€—æ—¶: {export_time:.2f}ç§’)')
            else:
                export_time = 0.0
                print(f' â­ï¸  è·³è¿‡å¯¼å‡º: {db_name} (ä½¿ç”¨å·²æœ‰å¯¼å‡ºæ•°æ®)')

            # å¯¼å…¥
            import_start_time = time.time()
            self.target_mongo.restore_db(database=db_name)
            import_time = time.time() - import_start_time
            print(f' âœ… æˆåŠŸå¯¼å…¥{self.target_config.host}: {db_name} (è€—æ—¶: {import_time:.2f}ç§’)')

            total_time = time.time() - start_time
            return db_name, True, total_time, export_time, import_time

        except Exception as e:
            total_time = time.time() - start_time
            print(f' âŒ å¤„ç†æ•°æ®åº“ {db_name} å¤±è´¥: {str(e)}')
            return db_name, False, total_time, export_time, import_time

    def cleanup_dump_folder(self) -> None:
        """æ¸…ç†å†å²å¯¼å‡ºç›®å½•"""
        if self.global_config.dump_root_path.exists():
            # åªåˆ é™¤ç›®å½•å†…å®¹ï¼Œä¸åˆ é™¤ç›®å½•æœ¬èº«ï¼ˆäº‘ç›˜æŒ‚è½½è·¯å¾„ï¼‰
            for item in self.global_config.dump_root_path.iterdir():
                if item.is_file():
                    item.unlink()
                elif item.is_dir():
                    shutil.rmtree(item)

    def dump_and_restore(self):

        # æ¸…ç†å†å²å¯¼å‡ºç›®å½•ï¼ˆä»…åœ¨éœ€è¦å¯¼å‡ºæ—¶ï¼‰
        if not self.global_config.skip_export:
            self.cleanup_dump_folder()
            self.global_config.dump_root_path.mkdir(exist_ok=True)
        else:
            print("âš ï¸  è·³è¿‡å¯¼å‡ºæ¨¡å¼ï¼Œä¿ç•™ç°æœ‰å¯¼å‡ºæ•°æ®")

        total_start_time = time.time()

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†æ¯ä¸ªæ•°æ®åº“
        successful_dbs = []
        failed_dbs = []

        with concurrent.futures.ThreadPoolExecutor(max_workers=self.global_config.maxThreads) as pool:
            # æäº¤æ‰€æœ‰æ•°æ®åº“å¤„ç†ä»»åŠ¡
            future_to_db = {
                pool.submit(self.process_single_database, db.strip()): db.strip()
                for db in self.global_config.databases
            }

            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            try:
                for future in concurrent.futures.as_completed(future_to_db):
                    db_name = future_to_db[future]
                    try:
                        db_name, success, duration, export_time, import_time = future.result()
                        if success:
                            successful_dbs.append((db_name, duration, export_time, import_time))
                            print(f' ğŸ‰ æ•°æ®åº“ {db_name} å¤„ç†å®Œæˆ (è€—æ—¶: {duration:.2f}ç§’)')
                        else:
                            failed_dbs.append((db_name, duration, export_time, import_time))
                            print(f' ğŸ’¥ æ•°æ®åº“ {db_name} å¤„ç†å¤±è´¥ (è€—æ—¶: {duration:.2f}ç§’)')

                    except KeyboardInterrupt:
                        print(f" âš ï¸  ç”¨æˆ·ä¸­æ–­å¤„ç†æ•°æ®åº“: {db_name}")
                        failed_dbs.append((db_name, 0, 0, 0))
                        break
                    except TimeoutError as e:
                        print(f" â° æ•°æ®åº“ {db_name} å¤„ç†è¶…æ—¶: {str(e)}")
                        failed_dbs.append((db_name, 0, 0, 0))
                    except Exception as e:
                        failed_dbs.append((db_name, 0, 0, 0))
                        print(f' ğŸ’¥ æ•°æ®åº“ {db_name} å¤„ç†æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}')
            except KeyboardInterrupt:
                print("\n âš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")
                # å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡
                for future in future_to_db:
                    future.cancel()
                print(" âœ… å·²å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡")

        total_time = time.time() - total_start_time

        # è®¡ç®—æ€»è®¡æ—¶é—´
        total_export_time = sum(db[2] for db in successful_dbs) + sum(db[2] for db in failed_dbs)
        total_import_time = sum(db[3] for db in successful_dbs) + sum(db[3] for db in failed_dbs)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“ˆ å¤„ç†å®Œæˆç»Ÿè®¡:")
        print(f"   âœ… æˆåŠŸ: {len(successful_dbs)}ä¸ªæ•°æ®åº“")
        for db_name, duration, export_time, import_time in successful_dbs:
            print(f"      - {db_name}: {duration:.2f}ç§’ (å¯¼å‡º: {export_time:.2f}ç§’, å¯¼å…¥: {import_time:.2f}ç§’)")

        if failed_dbs:
            print(f"   âŒ å¤±è´¥: {len(failed_dbs)}ä¸ªæ•°æ®åº“")
            for db_name, duration, export_time, import_time in failed_dbs:
                print(f"      - {db_name}: {duration:.2f}ç§’ (å¯¼å‡º: {export_time:.2f}ç§’, å¯¼å…¥: {import_time:.2f}ç§’)")

        print(f"\nğŸ“Š æ€»è®¡ç»Ÿè®¡:")
        print(f"   æ€»å¯¼å‡ºæ—¶é—´: {total_export_time:.2f}ç§’")
        print(f"   æ€»å¯¼å…¥æ—¶é—´: {total_import_time:.2f}ç§’")
        print(f"   æ€»å¤„ç†æ—¶é—´: {total_time:.2f}ç§’")
        if total_export_time + total_import_time > 0:
            print(f"   å¹¶è¡Œæ•ˆç‡: {total_time / (total_export_time + total_import_time):.2f}x")
        else:
            print(f"   å¹¶è¡Œæ•ˆç‡: N/A")

        print(f' ğŸ¯ æ‰€æœ‰æ•°æ®åº“æ“ä½œå®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’')

        # å¦‚æœæœ‰å¤±è´¥çš„æ•°æ®åº“ï¼Œæ‰“å°é”™è¯¯ä¿¡æ¯
        if failed_dbs:
            print("âš ï¸  éƒ¨åˆ†æ•°æ®åº“å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
            print("   å¤±è´¥çš„æ•°æ®åº“:")
            for db_name, duration, export_time, import_time in failed_dbs:
                print(f"      - {db_name}")

        # ç¨‹åºç»“æŸ
        # ä¸ºæˆåŠŸçš„æ•°æ®åº“åˆ›å»ºç´¢å¼•
        if successful_dbs:
            print("\nğŸ” å¼€å§‹åˆ›å»ºç´¢å¼•...")
            print("=" * 50)

            try:
                # è·å–æˆåŠŸçš„æ•°æ®åº“ååˆ—è¡¨
                successful_db_names = [db[0] for db in successful_dbs]

                # å¹¶å‘åˆ›å»ºç´¢å¼•
                index_results = self.recreate_indexes_for_databases(
                    successful_db_names
                )

                # æ‰“å°ç´¢å¼•åˆ›å»ºç»Ÿè®¡
                print("\n" + "=" * 50)
                print(f"ğŸ“Š ç´¢å¼•åˆ›å»ºæ±‡æ€»ç»Ÿè®¡:")
                print(f"   ğŸ“‚ å¤„ç†æ•°æ®åº“æ•°: {index_results['total_databases']}")
                print(f"   âœ… æˆåŠŸæ•°æ®åº“æ•°: {index_results['successful_databases']}")
                print(f"   ğŸ“ˆ æ€»ç´¢å¼•æ•°: {index_results['total_indexes']}")
                print(f"   ğŸ¯ æˆåŠŸåˆ›å»ºç´¢å¼•æ•°: {index_results['created_indexes']}")
                print(f"   â±ï¸  æ€»è€—æ—¶: {index_results['duration']:.2f}ç§’")

                if index_results['failed_databases']:
                    print(f"\n   âŒ å¤±è´¥æ•°æ®åº“:")
                    for failed_db in index_results['failed_databases']:
                        print(f"      ğŸ“› {failed_db['database']}: {failed_db.get('error', 'æœªçŸ¥é”™è¯¯')}")
                else:
                    print(f"\n   ğŸ‰ æ‰€æœ‰æ•°æ®åº“ç´¢å¼•åˆ›å»ºæˆåŠŸ!")

            except Exception as e:
                print(f"âš ï¸  ç´¢å¼•åˆ›å»ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

        print("âœ… æ•°æ®å¯¼å‡ºå¯¼å…¥ç¨‹åºæ‰§è¡Œå®Œæˆ")

    def recreate_indexes_for_database(self, database_name: str) -> Dict[str, Any]:
        """
        é‡æ–°åˆ›å»ºå•ä¸ªæ•°æ®åº“çš„æ‰€æœ‰ç´¢å¼•

        Args:
            database_name: æ•°æ®åº“åç§°

        Returns:
            åŒ…å«ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        start_time = time.time()

        try:
            # è·å–æºæ•°æ®åº“çš„ç´¢å¼•ä¿¡æ¯
            indexes_info = self.source_mongo.get_database_indexes(database_name)

            if not indexes_info:
                return {
                    'database': database_name,
                    'success': True,
                    'total_indexes': 0,
                    'created_indexes': 0,
                    'failed_collections': [],
                    'duration': time.time() - start_time
                }

            # è®¡ç®—æ€»ç´¢å¼•æ•°
            total_indexes = sum(len(indexes) for indexes in indexes_info.values())

            if total_indexes == 0:
                return {
                    'database': database_name,
                    'success': True,
                    'total_indexes': 0,
                    'created_indexes': 0,
                    'failed_collections': [],
                    'duration': time.time() - start_time
                }

            # åœ¨ç›®æ ‡æ•°æ®åº“ä¸Šåˆ›å»ºç´¢å¼•
            results = self.target_mongo.create_indexes_on_target(database_name, indexes_info)

            # ç»Ÿè®¡å¤±è´¥çš„é›†åˆ
            failed_collections = [col for col, success in results.items() if not success]
            created_indexes = total_indexes - len(failed_collections)

            return {
                'database': database_name,
                'success': len(failed_collections) == 0,
                'total_indexes': total_indexes,
                'created_indexes': created_indexes,
                'failed_collections': failed_collections,
                'duration': time.time() - start_time
            }

        except Exception as e:
            print(f"âŒ æ•°æ®åº“ {database_name} ç´¢å¼•åˆ›å»ºå¤±è´¥: {e}")
            return {
                'database': database_name,
                'success': False,
                'total_indexes': 0,
                'created_indexes': 0,
                'failed_collections': [],
                'duration': time.time() - start_time,
                'error': str(e)
            }

    def recreate_indexes_for_databases(self, database_names: List[str]) -> Dict[str, Any]:
        """
        å¹¶å‘é‡æ–°åˆ›å»ºå¤šä¸ªæ•°æ®åº“çš„ç´¢å¼•

        Args:
            database_names: æ•°æ®åº“åç§°åˆ—è¡¨

        Returns:
            åŒ…å«æ€»ä½“ç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸
        """
        start_time = time.time()
        total_databases = len(database_names)

        if total_databases == 0:
            return {
                'total_databases': 0,
                'successful_databases': 0,
                'failed_databases': [],
                'total_indexes': 0,
                'created_indexes': 0,
                'duration': 0
            }

        results = []

        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
        with ThreadPoolExecutor(max_workers=self.global_config.maxThreads) as executor:
            future_to_db = {
                executor.submit(self.recreate_indexes_for_database, db_name): db_name
                for db_name in database_names
            }

            for future in as_completed(future_to_db):
                db_name = future_to_db[future]
                try:
                    result = future.result()
                    results.append(result)

                    if result['success']:
                        print(f"âœ… æ•°æ®åº“ {db_name} ç´¢å¼•åˆ›å»ºå®Œæˆ "
                              f"(æ€»ç´¢å¼•: {result['total_indexes']}, "
                              f"æˆåŠŸ: {result['created_indexes']}, "
                              f"è€—æ—¶: {result['duration']:.2f}ç§’)")
                    else:
                        print(f"âš ï¸  æ•°æ®åº“ {db_name} ç´¢å¼•åˆ›å»ºå¤±è´¥ "
                              f"(å¤±è´¥é›†åˆ: {result.get('failed_collections', [])})")

                except Exception as e:
                    results.append({
                        'database': db_name,
                        'success': False,
                        'total_indexes': 0,
                        'created_indexes': 0,
                        'failed_collections': [],
                        'duration': 0,
                        'error': str(e)
                    })

        # æ±‡æ€»ç»Ÿè®¡
        successful_databases = [r for r in results if r['success']]
        failed_databases = [r for r in results if not r['success']]

        total_indexes = sum(r['total_indexes'] for r in results)
        created_indexes = sum(r['created_indexes'] for r in results)

        return {
            'total_databases': total_databases,
            'successful_databases': len(successful_databases),
            'failed_databases': failed_databases,
            'total_indexes': total_indexes,
            'created_indexes': created_indexes,
            'duration': time.time() - start_time,
            'details': results
        }
