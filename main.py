#!/usr/bin/env python3
import os
import platform
import shutil
import sys
import time
import concurrent.futures
import subprocess
from configparser import ConfigParser
from pathlib import Path

from typing import List, Tuple
from dump import MyDump, MyImport, Mongo


def cleanup_dump_folder(dump_folder: Path) -> None:
    """æ¸…ç†å†å²å¯¼å‡ºç›®å½•"""
    if dump_folder.exists():
        # åªåˆ é™¤ç›®å½•å†…å®¹ï¼Œä¸åˆ é™¤ç›®å½•æœ¬èº«ï¼ˆäº‘ç›˜æŒ‚è½½è·¯å¾„ï¼‰
        for item in dump_folder.iterdir():
            if item.is_file():
                item.unlink()
            elif item.is_dir():
                shutil.rmtree(item)


def get_large_collections_info(source: Mongo, database: str,
                             threshold_docs: int = 1000000) -> List[Tuple[str, int]]:
    """
    è·å–æ•°æ®åº“ä¸­çš„å¤§é›†åˆä¿¡æ¯

    :param source: MongoDBè¿æ¥ä¿¡æ¯
    :param database: æ•°æ®åº“å
    :param threshold_docs: å¤§é›†åˆé˜ˆå€¼
    :return: [(é›†åˆå, æ–‡æ¡£æ•°é‡), ...]
    """
    auth_append = f'--username={source.username} --password="{source.password}" --authenticationDatabase=admin' if source.username else ''

    try:
        # è·å–æ‰€æœ‰é›†åˆ
        collections_cmd = f'mongo --host="{source.host}:{source.port}" {auth_append} --quiet --eval "db.getSiblingDB(\'{database}\').getCollectionNames()" --norc'
        result = subprocess.run(collections_cmd, shell=True, capture_output=True, text=True)
        collections = eval(result.stdout.strip())

        large_collections = []
        for collection in collections:
            # è·å–æ–‡æ¡£æ•°é‡
            count_cmd = f'mongo --host="{source.host}:{source.port}" {auth_append} --quiet --eval "db.getSiblingDB(\'{database}\').{collection}.countDocuments()" --norc'
            count_result = subprocess.run(count_cmd, shell=True, capture_output=True, text=True)
            try:
                doc_count = int(count_result.stdout.strip())
                if doc_count >= threshold_docs:
                    large_collections.append((collection, doc_count))
            except:
                continue

        return large_collections
    except Exception as e:
        print(f"âš ï¸  è·å–å¤§é›†åˆä¿¡æ¯å¤±è´¥: {str(e)}")
        return []


def process_single_database(db_name: str, source: Mongo, target: Mongo,
                            numParallelCollections: int, numInsertionWorkersPerCollection: int, dump_folder: Path,
                            large_collection_threshold: int = 1000000) -> Tuple[str, bool, float]:
    """
    å¤„ç†å•ä¸ªæ•°æ®åº“çš„å¯¼å‡ºã€å¯¼å…¥å’Œæ¸…ç†
    :param db_name: æ•°æ®åº“åç§°
    :param source: æºMongoDBè¿æ¥ä¿¡æ¯
    :param target: ç›®æ ‡MongoDBè¿æ¥ä¿¡æ¯
    :param numParallelCollections: å¹¶å‘æ•°
    :param numInsertionWorkersPerCollection: æ¯ä¸ªé›†åˆçš„æ’å…¥å·¥ä½œçº¿ç¨‹æ•°
    :param dump_folder: å¯¼å‡ºç›®å½•
    :param large_collection_threshold: å¤§é›†åˆé˜ˆå€¼ï¼Œè¶…è¿‡æ­¤å€¼ä½¿ç”¨åˆ†åŒºå¯¼å‡º
    :return: (æ•°æ®åº“å, æ˜¯å¦æˆåŠŸ, æ€»è€—æ—¶)
    """
    start_time = time.time()
    try:
        # è·å–å¤§é›†åˆä¿¡æ¯
        large_collections = get_large_collections_info(source, db_name, large_collection_threshold)

        export_start_time = time.time()

        if large_collections:
            print(f' â„¹ï¸ ä»{source.host}å¯¼å‡º: {db_name} (æ£€æµ‹åˆ° {len(large_collections)} ä¸ªå¤§é›†åˆ)')

            # å¤„ç†å¤§é›†åˆçš„åˆ†åŒºå¯¼å‡º
            mydump = MyDump(source, numParallelCollections)
            all_partition_dirs = []

            for collection_name, doc_count in large_collections:
                print(f'   ğŸ“Š å¤§é›†åˆ {collection_name}: {doc_count:,} æ¡æ–‡æ¡£ï¼Œä½¿ç”¨åˆ†åŒºå¯¼å‡º')

                # è®¡ç®—åˆ†åŒºæ•°é‡ï¼šæ¯50ä¸‡æ¡ä¸€ä¸ªåˆ†åŒºï¼Œæœ€å¤š8ä¸ª
                partitions = min(8, max(2, doc_count // 500000))

                # åˆ†åŒºå¯¼å‡ºå¤§é›†åˆ
                partition_dirs = mydump.export_collection_partitioned(
                    database=db_name,
                    collection=collection_name,
                    dump_root_path=str(dump_folder),
                    partition_field="_id",
                    partitions=partitions
                )
                all_partition_dirs.extend(partition_dirs)

            # æ™®é€šå¯¼å‡ºå‰©ä½™çš„å°é›†åˆ
            print(f'   ğŸ“Š å¯¼å‡ºå‰©ä½™å°é›†åˆ...')
            auth_append = f'--username={source.username} --password="{source.password}" --authenticationDatabase=admin' if source.username else ''

            # æ ¹æ®å¹³å°é€‰æ‹©æ­£ç¡®çš„mongodumpè·¯å¾„
            if platform.system() == 'Windows':
                mongodump_exe = os.path.join('mongodb-database-tools', 'windows-x86_64-100.13.0', 'mongodump.exe')
            elif platform.system() == 'Linux':
                mongodump_exe = os.path.join('mongodb-database-tools', 'rhel93-x86_64-100.13.0', 'mongodump')
            elif platform.system() == 'Darwin':
                mongodump_exe = os.path.join('mongodb-database-tools', 'macos-arm64-100.13.0', 'mongodump')

            export_cmd = (
                f'{mongodump_exe} '
                f'--host="{source.host}:{source.port}" '
                f'--db={db_name} '
                f'--out={dump_folder} '
                f'--numParallelCollections={numParallelCollections} '
                f'--gzip {auth_append} '
            )

            # æ’é™¤å·²å¤„ç†çš„å¤§é›†åˆ
            exclude_collections = ' '.join([f'--excludeCollection={collection_name}' for collection_name, _ in large_collections])
            export_cmd += exclude_collections

            mydump._exe_command(export_cmd)

        else:
            # æ²¡æœ‰å¤§é›†åˆï¼Œä½¿ç”¨æ ‡å‡†å¯¼å‡º
            print(f' â„¹ï¸ ä»{source.host}å¯¼å‡º: {db_name} (æ— å¤§é›†åˆï¼Œä½¿ç”¨æ ‡å‡†å¯¼å‡º)')
            mydump = MyDump(source, numParallelCollections)
            mydump.export_db(db_name, dump_folder)

        export_time = time.time() - export_start_time
        print(f' âœ… æˆåŠŸä»{source.host}å¯¼å‡º: {db_name} (è€—æ—¶: {export_time:.2f}ç§’)')

        db_dir = os.path.join(dump_folder, db_name)

        # å¯¼å…¥uat
        print(f' â„¹ï¸ å¯¼å…¥{target.host}: {db_name}')
        import_start_time = time.time()

        if large_collections:
            # å¤„ç†åˆ†åŒºå¯¼å‡ºçš„å¯¼å…¥
            myimport = MyImport(target, numParallelCollections, numInsertionWorkersPerCollection)

            # å¯¼å…¥æ™®é€šé›†åˆ
            if os.path.exists(db_dir):
                myimport.import_db(db_name, db_dir)

            # å¹¶å‘å¯¼å…¥å¤§é›†åˆçš„åˆ†åŒº
            for collection_name, _ in large_collections:
                partition_dirs = []
                for i in range(8):  # æœ€å¤šæ£€æŸ¥8ä¸ªåˆ†åŒº
                    partition_dir = os.path.join(dump_folder, f"{db_name}_{collection_name}_part{i}")
                    if os.path.exists(partition_dir):
                        partition_dirs.append(partition_dir)

                if partition_dirs:
                    print(f'   ğŸ”„ å¹¶å‘å¯¼å…¥å¤§é›†åˆ {collection_name} çš„ {len(partition_dirs)} ä¸ªåˆ†åŒº...')
                    myimport.import_partitioned_collection(db_name, partition_dirs)

        else:
            # æ ‡å‡†å¯¼å…¥
            myimport = MyImport(target, numParallelCollections, numInsertionWorkersPerCollection)
            myimport.import_db(db_name, db_dir)

        import_time = time.time() - import_start_time
        print(f' âœ… æˆåŠŸå¯¼å…¥{target.host}: {db_name} (è€—æ—¶: {import_time:.2f}ç§’)')

        # åˆ é™¤å¯¼å‡ºçš„æ–‡ä»¶
        print(f' âœ… åˆ é™¤ä¸´æ—¶æ–‡ä»¶ç¼“å­˜: {db_dir}')
        if os.path.exists(db_dir):
            shutil.rmtree(db_dir)

        total_time = time.time() - start_time
        return db_name, True, total_time

    except Exception as e:
        total_time = time.time() - start_time
        print(f' âŒ å¤„ç†æ•°æ®åº“ {db_name} å¤±è´¥: {str(e)}')
        return db_name, False, total_time


def main():
    """ä¸»å‡½æ•° - ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†"""
    config = ConfigParser()
    config.read('config.ini')

    source = Mongo(
        config.get('source', 'host'),
        config.get('source', 'port'),
        config.get('source', 'username'),
        config.get('source', 'password')
    )

    target = Mongo(
        config.get('target', 'host'),
        config.get('target', 'port'),
        config.get('target', 'username'),
        config.get('target', 'password')
    )

    databases = config.get('global', 'databases').split(',')
    maxThreads = config.getint('global', 'maxThreads', fallback=4)  # æ–°å¢é…ç½®é¡¹
    numParallelCollections = config.getint('global', 'numParallelCollections')
    numInsertionWorkersPerCollection = config.getint('global', 'numInsertionWorkersPerCollection')
    large_collection_threshold = config.getint('global', 'largeCollectionThreshold', fallback=1000000)  # å¤§é›†åˆé˜ˆå€¼

    dump_folder = Path(__file__).parent / 'dumps'

    # æ¸…ç†å†å²å¯¼å‡ºç›®å½•
    cleanup_dump_folder(dump_folder)
    dump_folder.mkdir(exist_ok=True)

    print(f"âš™ï¸ å¯¼å‡ºé…ç½®: å•åº“å¹¶å‘æ•°={numParallelCollections}, çº¿ç¨‹æ± å¹¶å‘æ•°={maxThreads}, å¤§é›†åˆé˜ˆå€¼={large_collection_threshold:,}")
    print(f"ğŸ“Š å¾…å¤„ç†æ•°æ®åº“: {len(databases)}ä¸ª")

    total_start_time = time.time()

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†æ¯ä¸ªæ•°æ®åº“
    successful_dbs = []
    failed_dbs = []

    with concurrent.futures.ThreadPoolExecutor(max_workers=maxThreads) as pool:
        # æäº¤æ‰€æœ‰æ•°æ®åº“å¤„ç†ä»»åŠ¡
        future_to_db = {
            pool.submit(process_single_database, db.strip(), source, target,
                        numParallelCollections, numInsertionWorkersPerCollection, dump_folder, large_collection_threshold): db.strip()
            for db in databases
        }

        # å¤„ç†å®Œæˆçš„ä»»åŠ¡
        try:
            for future in concurrent.futures.as_completed(future_to_db):
                db_name = future_to_db[future]
                try:
                    db_name, success, duration = future.result()
                    if success:
                        successful_dbs.append((db_name, duration))
                        print(f' ğŸ‰ æ•°æ®åº“ {db_name} å¤„ç†å®Œæˆ (è€—æ—¶: {duration:.2f}ç§’)')
                    else:
                        failed_dbs.append((db_name, duration))
                        print(f' ğŸ’¥ æ•°æ®åº“ {db_name} å¤„ç†å¤±è´¥ (è€—æ—¶: {duration:.2f}ç§’)')

                except KeyboardInterrupt:
                    print(f" âš ï¸  ç”¨æˆ·ä¸­æ–­å¤„ç†æ•°æ®åº“: {db_name}")
                    failed_dbs.append((db_name, 0))
                    break
                except TimeoutError as e:
                    print(f" â° æ•°æ®åº“ {db_name} å¤„ç†è¶…æ—¶: {str(e)}")
                    failed_dbs.append((db_name, 0))
                except Exception as e:
                    failed_dbs.append((db_name, 0))
                    print(f' ğŸ’¥ æ•°æ®åº“ {db_name} å¤„ç†æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}')
        except KeyboardInterrupt:
            print("\n âš ï¸  æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¼˜é›…é€€å‡º...")
            # å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡
            for future in future_to_db:
                future.cancel()
            print(" âœ… å·²å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡")

    total_time = time.time() - total_start_time

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“ˆ å¤„ç†å®Œæˆç»Ÿè®¡:")
    print(f"   âœ… æˆåŠŸ: {len(successful_dbs)}ä¸ªæ•°æ®åº“")
    for db_name, duration in successful_dbs:
        print(f"      - {db_name}: {duration:.2f}ç§’")

    if failed_dbs:
        print(f"   âŒ å¤±è´¥: {len(failed_dbs)}ä¸ªæ•°æ®åº“")
        for db_name, duration in failed_dbs:
            print(f"      - {db_name}: {duration:.2f}ç§’")

    print(f' ğŸ¯ æ‰€æœ‰æ•°æ®åº“æ“ä½œå®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’')

    # å¦‚æœæœ‰å¤±è´¥çš„æ•°æ®åº“ï¼Œé€€å‡ºç ä¸ºé0
    if failed_dbs:
        print("âš ï¸  éƒ¨åˆ†æ•°æ®åº“å¤„ç†å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ—¥å¿—")
        # ä¸å¼ºåˆ¶é€€å‡ºï¼Œè®©ç”¨æˆ·é€‰æ‹©æ˜¯å¦é‡è¯•
        retry = input("æ˜¯å¦é‡è¯•å¤±è´¥çš„æ•°æ®åº“ï¼Ÿ(y/n): ").lower().strip()
        if retry == 'y':
            # é‡æ–°å¤„ç†å¤±è´¥çš„æ•°æ®åº“
            failed_db_names = [db[0] for db in failed_dbs]
            print(f"é‡æ–°å¤„ç†å¤±è´¥çš„æ•°æ®åº“: {failed_db_names}")
            # è¿™é‡Œå¯ä»¥æ·»åŠ é‡è¯•é€»è¾‘

    # ç¨‹åºç»“æŸ
    print("ğŸ’¤ ç¨‹åºæ‰§è¡Œå®Œæˆï¼Œè¿›å…¥ä¼‘çœ çŠ¶æ€...")

    try:
        while True:
            time.sleep(3600)  # æ¯å°æ—¶æ£€æŸ¥ä¸€æ¬¡
    except KeyboardInterrupt:
        print("æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œç¨‹åºç»“æŸ")
        sys.exit(0)


if __name__ == "__main__":
    main()
